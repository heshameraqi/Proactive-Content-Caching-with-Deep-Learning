{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "simulate_cache.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhpvrmO6AXKW"
      },
      "source": [
        "!git clone https://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@github.com/heshameraqi/Federated-Deep-Learning-for-Predictive-Content-Caching.git\n",
        "%cd Federated-Deep-Learning-for-Predictive-Content-Caching"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZMKKorR2gim"
      },
      "source": [
        "import logging, os\n",
        "logging.disable(logging.WARNING)\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from data import MovieLensData\n",
        "import math\n",
        "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from CFModel import CFModel, NCFModel  # Import Collaborative Filtering model architecture\n",
        "from math import floor\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SimModel:\n",
        "    def __init__(self, K_factor):\n",
        "        self.K_factor = K_factor # The number of dimensional embeddings for movies and users in the CF deep learning model\n",
        "        self.data = MovieLensData()\n",
        "        # Load data and print statistics\n",
        "        self.data.print_statistics()\n",
        "        # self.data.remove_movie_gap()  # TODO: uncomment this\n",
        "        self.users = self.data.ratings['user_emb_id'].values\n",
        "        self.movies = self.data.ratings['movie_emb_id'].values\n",
        "        # self.movies = [1,2,1,2,1,2,1,2]\n",
        "        # self.movies = [1,2,1,2,3,4,3,4]\n",
        "        self.ratings = self.data.ratings['rating'].values\n",
        "        # print(\"self.data.max_userid: \", self.data.max_userid, \" / self.data.max_movieid: \", self.data.max_movieid)\n",
        "\n",
        "    def train_ncf_model(self, nb_step, cf_flag=True):\n",
        "        # the default model will be ncf, cf will be used instead only if specified\n",
        "        len_step = floor(len(self.movies)/nb_step)\n",
        "        # we cut the ratings file into multiple intervals where the model will be trained in one interval and used in the next one\n",
        "        for i in range(0, nb_step):\n",
        "            print(\"=============== Training stage %d ===============\"%(i))\n",
        "            inf_index = len_step * i\n",
        "            # inf_index = 0  # in case we want to create intervals by going back each time to the beginning of the file\n",
        "            if i == (nb_step-1):\n",
        "                sup_index = len(self.movies)  # in case some ratings were left when the file was divided into multiple intervals\n",
        "            else:\n",
        "                sup_index = len_step * (i+1)\n",
        "            par_users = self.users[inf_index:sup_index]\n",
        "            par_movies = self.movies[inf_index:sup_index]\n",
        "            par_ratings = self.ratings[inf_index:sup_index]\n",
        "            if cf_flag:\n",
        "                model = CFModel(self.data.max_userid, self.data.max_movieid, K_FACTORS).model\n",
        "            else:\n",
        "                model = NCFModel(self.data.max_userid, self.data.max_movieid, K_FACTORS).model\n",
        "            model.compile(loss='mse', optimizer='adamax')\n",
        "            # Callbacks monitor the validation loss, save the model weights each time the validation loss has improved\n",
        "            callbacks = [EarlyStopping('val_loss', patience=10), ModelCheckpoint(f'weights{i+1}.h5', save_best_only=True)]\n",
        "            # Train the model: Use 30 epochs, 90% training data, 10% validation data\n",
        "            inputs = np.transpose(np.vstack((par_users, par_movies)))\n",
        "            history = model.fit(inputs, par_ratings, epochs=30, validation_split=.1, shuffle=True, batch_size=500, verbose=2, callbacks=callbacks)\n",
        "            # Plot training and validation RMSE\n",
        "            # loss = pd.DataFrame({'epoch': [ i + 1 for i in history.epoch ], 'training': [ math.sqrt(loss) for loss in history.history['loss'] ], 'validation': [ math.sqrt(loss) for loss in history.history['val_loss'] ]})\n",
        "            # ax = loss.ix[:,:].plot(x='epoch', figsize={7,10}, grid=True)\n",
        "            # ax.set_ylabel(\"root mean squared error\")\n",
        "            # ax.set_ylim([0.0,3.0]);\n",
        "            # Show the best validation RMSE\n",
        "            min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
        "            print('Minimum RMSE at epoch', '{:d}'.format(idx + 1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))\n",
        "\n",
        "    def apply_ncf_model(self, weights_file, cf_flag=True):\n",
        "        # the default model will be ncf, cf will be used instead only if specified\n",
        "        if cf_flag:\n",
        "            trained_model = CFModel(self.data.max_userid, self.data.max_movieid, K_FACTORS)\n",
        "            # Load weights\n",
        "            trained_model.model.load_weights(weights_file)\n",
        "        else:\n",
        "            trained_model = NCFModel(self.data.max_userid, self.data.max_movieid, K_FACTORS)\n",
        "            trained_model.model.load_weights(weights_file)\n",
        "        rec_movies_list_all = list()\n",
        "        # If a movie is recommended for more than X (threshold) users, it will be cached\n",
        "        # For every user TODO: should be every user in the past stages only not future\n",
        "        # TODO: Batch the data to the model\n",
        "        for i in range(1, len(self.users)+1):\n",
        "            # Predict user i ratings (enter user and his recommended movies --> get rating)\n",
        "            # print(\"data.ratings: \\n\", data.ratings)\n",
        "            # user_ratings = self.data.ratings[self.data.ratings['user_id'] == i][['user_id', 'movie_id', 'rating']]\n",
        "            # user_ratings['prediction'] = user_ratings.apply(lambda x: trained_model.rate(i, x['movie_id']), axis=1)\n",
        "            # user_ratings = user_ratings.sort_values(by='rating', ascending=False).merge(self.data.movies, on='movie_id', how='inner', suffixes=['_u', '_m']).head(20)\n",
        "            # print(user_ratings)\n",
        "            # Recommend user items (enter user and all movies --> get rating and sort them by best)\n",
        "            # Remove from data.ratings the movies already rated/requested by the user and predict from the list of movies not yet rated\n",
        "            user_ratings = self.data.ratings[self.data.ratings['user_id'] == i][['user_id', 'movie_id', 'rating']]\n",
        "            recommendations = self.data.ratings[self.data.ratings['movie_id'].isin(user_ratings['movie_id']) == False][['movie_id']].drop_duplicates()\n",
        "            recommendations['prediction'] = recommendations.apply(lambda x: trained_model.rate(i, x['movie_id']), axis=1)\n",
        "            recommendations = recommendations.sort_values(by='prediction', ascending=False).merge(self.data.movies, on='movie_id', how='inner', suffixes=['_u', '_m']).head(20)\n",
        "            # print(recommendations)\n",
        "            rec_movies_list_user = recommendations[\"movie_id\"].tolist()\n",
        "            rec_movies_list_all.append(rec_movies_list_user)\n",
        "            # print(recommended_movies_list)\n",
        "        return rec_movies_list_all\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configurations\n",
        "    # test_ratio = 0.1\n",
        "    # batch_size = 256\n",
        "    # split_mode = 'seq-aware'  # seq-aware or random\n",
        "    # feedback = 'explicit'  # explicit or implicit\n",
        "    K_FACTORS = 100\n",
        "    nb_step = 4\n",
        "    # cache_size = 10\n",
        "    sim_model = SimModel(K_FACTORS)\n",
        "    sim_model.train_ncf_model(nb_step)\n",
        "    '''for i in range(0, nb_step):\n",
        "        print(\"=============== Testing stage %d ===============\"%(i))\n",
        "        rec_movies_list_all = sim_model.apply_ncf_model(f'weights{i + 1}.h5')  # contains the recommended movies for each user\n",
        "        flat_rec_list = list(dict.fromkeys([item for sublist in rec_movies_list_all for item in sublist]))  # flat list of recommended movies\n",
        "        # print(rec_movies_list_all)'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}