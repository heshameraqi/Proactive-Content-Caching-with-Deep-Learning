{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTOOEYmhVP5x"
      },
      "source": [
        "from data import MovieLensData\n",
        "import math\n",
        "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
        "from CFModel import CFModel  # Import Collaborative Filtering model architecture\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras import backend as K\n",
        "\n",
        "# Configurations\n",
        "test_ratio = 0.1\n",
        "batch_size = 256\n",
        "split_mode = 'seq-aware'  # seq-aware or random\n",
        "feedback = 'explicit'  # explicit or implicit\n",
        "K_FACTORS = 100  # The number of dimensional embeddings for movies and users in the CF deep learning model\n",
        "\n",
        "# Use GPU for Kerasimport tensorflow as tf\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "config = tf.ConfigProto(device_count={'GPU':1, 'CPU':56})\n",
        "sess = tf.Session(config=config)\n",
        "keras.backend.set_session(sess)\n",
        "\n",
        "# Load data and print statistics\n",
        "data = MovieLensData()\n",
        "data.print_statistics()\n",
        "\n",
        "# Shuffle data\n",
        "shuffled_ratings = data.ratings.sample(frac=1., random_state=50)\n",
        "users = shuffled_ratings['user_emb_id'].values\n",
        "movies = shuffled_ratings['movie_emb_id'].values\n",
        "ratings = shuffled_ratings['rating'].values\n",
        "\n",
        "# Define model\n",
        "model = CFModel(data.max_userid, data.max_movieid, K_FACTORS)\n",
        "# model_ncf = NCFModel(data.max_userid, data.max_movieid, K_FACTORS)\n",
        "# model = model_ncf.model_final\n",
        "model.compile(loss='mse', optimizer='adamax')\n",
        "\n",
        "# Callbacks monitor the validation loss, save the model weights each time the validation loss has improved\n",
        "callbacks = [EarlyStopping('val_loss', patience=2), ModelCheckpoint('weights.h5', save_best_only=True)]\n",
        "\n",
        "# Train the model: Use 30 epochs, 90% training data, 10% validation data\n",
        "history = model.fit([users, movies], ratings, nb_epoch=30, validation_split=.1, verbose=2, callbacks=callbacks)\n",
        "\n",
        "# Show the best validation RMSE\n",
        "min_val_loss, idx = min((val, idx) for (idx, val) in enumerate(history.history['val_loss']))\n",
        "print('Minimum RMSE at epoch', '{:d}'.format(idx+1), '=', '{:.4f}'.format(math.sqrt(min_val_loss)))\n",
        "\n",
        "# Use the pre-trained model\n",
        "trained_model = CFModel(data.max_userid, data.max_movieid, K_FACTORS)\n",
        "# trained_model_obj = NCFModel(data.max_userid, data.max_movieid, K_FACTORS)\n",
        "# trained_model = trained_model_obj.model_final\n",
        "# Load weights\n",
        "trained_model.load_weights('weights.h5')\n",
        "\n",
        "# Predict user ratings (enter user and his recommended movies --> get rating)\n",
        "TEST_USER = 2000\n",
        "data.users[data.users['user_id'] == TEST_USER]\n",
        "\n",
        "def predict_rating(user_id, movie_id):\n",
        "    return trained_model.rate(user_id - 1, movie_id - 1)\n",
        "\t# return trained_model_obj.rate(user_id - 1, movie_id - 1)\n",
        "\n",
        "# user_ratings = data.ratings[ratings['user_id'] == TEST_USER][['user_id', 'movie_id', 'rating']]\n",
        "user_ratings = data.ratings[data.ratings['user_id'] == TEST_USER][['user_id', 'movie_id', 'rating']]\n",
        "user_ratings['prediction'] = user_ratings.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
        "# user_ratings.sort_values(by='rating', ascending=False).merge(movies, on='movie_id', how='inner', suffixes=['_u', '_m']).head(20)\n",
        "user_ratings = user_ratings.sort_values(by='rating', ascending=False).merge(data.movies, on='movie_id', how='inner', suffixes=['_u', '_m']).head(20)\n",
        "print(user_ratings)\n",
        "\n",
        "# Recommend user items (enter user and all movies --> get rating and sort them by best)\n",
        "recommendations = data.ratings[data.ratings['movie_id'].isin(user_ratings['movie_id']) == False][['movie_id']].drop_duplicates()\n",
        "recommendations['prediction'] = recommendations.apply(lambda x: predict_rating(TEST_USER, x['movie_id']), axis=1)\n",
        "# recommendations.sort_values(by='prediction', ascending=False).merge(movies, on='movie_id', how='inner', suffixes=['_u', '_m']).head(20)\n",
        "recommendations = recommendations.sort_values(by='prediction', ascending=False).merge(data.movies, on='movie_id', how='inner', suffixes=['_u', '_m']).head(20)\n",
        "print(recommendations)\n",
        "\n",
        "# Afterwards, we put the above steps together and it will be used in the next section. The results are wrapped with Dataset and DataLoader.\n",
        "# Note that the last_batch of DataLoader for training data is set to the rollover mode (The remaining samples are rolled over to the next epoch.) and orders are shuffled.\n",
        "# (train_u, train_i, train_r, train_inter), (test_u, test_i, test_r, test_inter) = data.split_load_data(split_mode, test_ratio, feedback)\n",
        "#train_set = data.ArrayDataset(np.array(train_u), np.array(train_i), np.array(train_r))\n",
        "#test_set = data.ArrayDataset(np.array(test_u), np.array(test_i), np.array(test_r))\n",
        "#train_iter = data.DataLoader(train_set, shuffle=True, last_batch='rollover', batch_size=batch_size)\n",
        "#test_iter = data.DataLoader(test_set, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}